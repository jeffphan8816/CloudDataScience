{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "from kafka import KafkaProducer, KafkaConsumer, TopicPartition\n",
    "import json\n",
    "import ast\n",
    "import logging\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EPA tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_kafka_data(data : dict) -> pd.DataFrame | None :\n",
    "    \"\"\"\n",
    "    Clean the data stored to a list of records, only keeping seeked columns\n",
    "\n",
    "    @returns a list of data dictionaries\n",
    "    \"\"\"\n",
    "    cleaned = []\n",
    "    # Go thorugh each record returned\n",
    "    if 'records' not in data.keys():\n",
    "        return None\n",
    "    \n",
    "    for record in data['records']:\n",
    "        # Pull out location\n",
    "        if 'geometry' not in record.keys():\n",
    "            continue\n",
    "        if 'coordinates' not in record['geometry'].keys():\n",
    "            continue\n",
    "        location = record['geometry']['coordinates']\n",
    "        # Then get the parameters\n",
    "        if 'parameters' not in record.keys():\n",
    "            continue\n",
    "        parameters = record['parameters']\n",
    "        for parameter in parameters:\n",
    "            # Pull out name\n",
    "            if 'name' not in parameter.keys():\n",
    "                continue\n",
    "            name = parameter['name']\n",
    "            if 'timeSeriesReadings' not in parameter.keys():\n",
    "                continue\n",
    "            for series in parameter['timeSeriesReadings']:\n",
    "                # Get all the readings\n",
    "                if 'readings' not in series.keys():\n",
    "                    continue\n",
    "                for reading in series['readings']:\n",
    "                    # Check for required data\n",
    "                    cont = True\n",
    "                    for key in ['since', 'until', 'averageValue']:\n",
    "                        if key not in reading.keys():\n",
    "                            cont = False\n",
    "                    if not cont:\n",
    "                        continue\n",
    "                    # Build dictionary and append\n",
    "                    toAdd = {}\n",
    "                    toAdd['measure_name'] = name\n",
    "                    toAdd['location'] = location\n",
    "                    toAdd['start'] = datetime.strptime(\n",
    "                        reading['since'], '%Y-%m-%dT%H:%M:%SZ')\n",
    "                    toAdd['end'] = datetime.strptime(\n",
    "                        reading['until'], '%Y-%m-%dT%H:%M:%SZ')\n",
    "                    toAdd['value'] = reading['averageValue']\n",
    "                    cleaned.append(toAdd)\n",
    "\n",
    "    df_new_data = pd.DataFrame.from_records(cleaned, index=range(len(cleaned)))\n",
    "\n",
    "    df_new_data['start'] = df_new_data['start'].dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    df_new_data['end'] = df_new_data['end'].dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "    df_new_data['start'] = df_new_data['start'].apply(lambda s: datetime.strptime(s, '%Y-%m-%dT%H:%M:%S'))\n",
    "    df_new_data['end'] = df_new_data['end'].apply(lambda s: datetime.strptime(s, '%Y-%m-%dT%H:%M:%S'))\n",
    "\n",
    "    # Switch coordinate order for new data and move to tuple\n",
    "    df_new_data['location'] = df_new_data['location'].apply(\n",
    "        lambda location: (location[1], location[0]))\n",
    "    \n",
    "    return df_new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_kafka_data({'not_records':0}) is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[{'measure_name': 'particule', 'location': ('lat', 'long'), 'start': Timestamp('2024-01-01 00:00:00'), 'end': Timestamp('2024-01-01 01:00:00'), 'value': 'value'}]\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'records':[{'geometry': {'coordinates':('long','lat')},\n",
    "                    'parameters':[{'name':'particule', \n",
    "                                  'timeSeriesReadings':[{'readings':[{'since':'2024-01-01T00:00:00Z',\n",
    "                                                                     'until':'2024-01-01T01:00:00Z',\n",
    "                                                                     'averageValue':'value'}]}]}]}]}\n",
    "str(clean_kafka_data(data).to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_string = \"[{'measure_name': 'particule', 'location': ('lat', 'long'), 'start': Timestamp('2024-01-01 00:00:00'), 'end': Timestamp('2024-01-01 01:00:00'), 'value': 'value'}]\"\n",
    "\n",
    "str(clean_kafka_data(data).to_dict(orient='records')) == result_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accepting_new_data(new_data: pd.DataFrame, current_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute which data to keep and upload, based on time range inclusion, \n",
    "    to prevent duplicatas\n",
    "\n",
    "    @param new_data is the data pulled from the EPA as a DataFrame\n",
    "    @param current_data is the data in elastic search as a DataFrame\n",
    "    @returns a list of what data needs to be inserted\n",
    "    \"\"\"\n",
    "    latest_current_df = current_data.groupby(['measure_name', 'location'])['end'].max()\n",
    "    kept_data = new_data.copy()\n",
    "\n",
    "    for index in new_data.index:\n",
    "\n",
    "        name = new_data.loc[index, 'measure_name']\n",
    "        # Need to convert coorinates to tuple\n",
    "        location = (new_data.loc[index, 'location'][0],\n",
    "                    new_data.loc[index, 'location'][1])\n",
    "\n",
    "        # Check for collisions\n",
    "        if name in latest_current_df.index:\n",
    "            if location in latest_current_df[new_data.loc[index, 'measure_name']].index:\n",
    "                if new_data.loc[index, 'end'] <= latest_current_df[new_data.loc[index, 'measure_name']][new_data.loc[index, 'location']]:\n",
    "                    kept_data = kept_data.drop(index, axis='index')\n",
    "    return kept_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measure_name</th>\n",
       "      <th>location</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P1</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P2</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  measure_name location  end\n",
       "0           P1   (0, 0)    2\n",
       "2           P2   (1, 1)    2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_records =  [{'measure_name': 'P1', 'location': (0,0), 'end': 0},\n",
    "                    {'measure_name': 'P1', 'location': (0,0), 'end': 1},\n",
    "                    {'measure_name': 'P1', 'location': (1,1), 'end': 1},\n",
    "                    {'measure_name': 'P2', 'location': (0,0), 'end': 1},\n",
    "                    {'measure_name': 'P2', 'location': (1,1), 'end': 1}]\n",
    "new_records =  [{'measure_name': 'P1', 'location': (0,0), 'end': 2},\n",
    "                {'measure_name': 'P2', 'location': (0,0), 'end': 1},\n",
    "                {'measure_name': 'P2', 'location': (1,1), 'end': 2}]\n",
    "current_data = pd.DataFrame(current_records)\n",
    "new_data = pd.DataFrame(new_records)\n",
    "\n",
    "accepting_new_data(new_data,current_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[{'measure_name': 'P1', 'location': (0, 0), 'end': 2}, {'measure_name': 'P2', 'location': (1, 1), 'end': 2}]\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_string = str(accepting_new_data(new_data,current_data).to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(accepting_new_data(new_data,current_data).to_dict(orient='records')) == result_string"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
