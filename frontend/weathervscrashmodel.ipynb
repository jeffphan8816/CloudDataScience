{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeffphan/PycharmProjects/cloudcompa2/.venv/lib/python3.8/site-packages/elasticsearch/_sync/client/__init__.py:399: SecurityWarning: Connecting to 'https://172.26.135.52:9200' using TLS with verify_certs=False is insecure\n",
      "  _transport = transport_class(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "import json\n",
    "import time\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import scan\n",
    "import copy\n",
    "from tmpFission import tempFissionStream, tempFissionCrashesFromStation, tempFissionWeather, tempMSearch\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "FISSION_URL = 'http://172.26.135.52:9090/'\n",
    "FISSION_HEADERS = {'HOST': 'fission'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_data(params):\n",
    "    # Define search query (optional, can be empty to retrieve all documents)\n",
    "    data = []\n",
    "    max_retries = 3\n",
    "    retry_delay = 5  # seconds\n",
    "    timeout = 60  # seconds\n",
    "\n",
    "    for _ in range(max_retries):\n",
    "        try:\n",
    "            res = requests.get(f\"{FISSION_URL}/{params}\", headers=FISSION_HEADERS, timeout=timeout)\n",
    "            if res.status_code != 200:\n",
    "                print(res.text)\n",
    "                return None\n",
    "            data = json.loads(res.text)\n",
    "            return data\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Connection error: {e}\")\n",
    "            print(\"Retrying in 5 seconds...\")\n",
    "            time.sleep(retry_delay)\n",
    "    \n",
    "    print(\"Max retries exceeded. Unable to retrieve data.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resp = get_full_data('stations')\n",
    "if resp is None:\n",
    "    print(\"Failed to retrieve data.\")\n",
    "elif resp['Status'] != 200:\n",
    "    print(f\"Failed to retrieve data. Status code: {resp['Status']}\")\n",
    "stations = pd.DataFrame(resp[\"Data\"])\n",
    "stations.head()\n",
    "stations_copy = stations.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #loop through all stations\n",
    "# size = 3000 #number of crashes to get\n",
    "# radius = 15 #radius in kms\n",
    "# crashes = {}\n",
    "# empty_stations = []\n",
    "# error_stations = []\n",
    "\n",
    "# for index, station in stations_copy.iterrows():\n",
    "#     station_id = station['Station ID']\n",
    "#     # station_id = \"91306\"\n",
    "#     crashes[station_id] = crashes.get(station_id, [])\n",
    "#     # params = f\"crashes/{station_id}/{size}/{radius}\"\n",
    "#     # resp = get_full_data(params)\n",
    "#     resp = tempFissionCrashesFromStation(station_id, size, radius)\n",
    "#     print(resp)\n",
    "#     # if resp is None or \"Data\" not in resp or \"Token\" not in resp or \"Status\" not in resp:\n",
    "#     #     error_stations.append(station_id)\n",
    "#     #     continue\n",
    "#     # elif resp[\"Data\"] == [] or resp[\"Token\"] == \"END\" or resp[\"Status\"] != 200:\n",
    "#     #     empty_stations.append(station_id)\n",
    "#     #     continue\n",
    "#     # else:\n",
    "#     #     crashes[station_id].extend(resp[\"Data\"])\n",
    "#     #     while resp and \"Token\" in resp and resp[\"Token\"] != \"END\":\n",
    "#     #         params = f\"stream/{resp['Token']}\"\n",
    "#     #         resp = get_full_data(params)\n",
    "#     #         if resp and resp[\"Data\"]:\n",
    "#     #             crashes[station_id].extend(resp[\"Data\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ApiError",
     "evalue": "ApiError(502, '<html>\\r\\n<head><title>502 Bad Gateway</title></head>\\r\\n<body>\\r\\n<center><h1>502 Bad Gateway</h1></center>\\r\\n<hr><center>nginx</center>\\r\\n</body>\\r\\n</html>\\r\\n')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mApiError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#get list of station ID from stations\u001b[39;00m\n\u001b[1;32m      5\u001b[0m station_ids \u001b[38;5;241m=\u001b[39m stations_copy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStation ID\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m----> 6\u001b[0m tempresp \u001b[38;5;241m=\u001b[39m \u001b[43mtempMSearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstation_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcrashes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mradius\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/cloudcompa2/frontend/tmpFission.py:88\u001b[0m, in \u001b[0;36mtempMSearch\u001b[0;34m(max_hits, stations, index, radius)\u001b[0m\n\u001b[1;32m     85\u001b[0m         msearch_body\u001b[38;5;241m.\u001b[39mappend(query)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Perform msearch to search for crashes near stations\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmsearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsearch_body\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m station_id, location \u001b[38;5;129;01min\u001b[39;00m station_locations\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/PycharmProjects/cloudcompa2/.venv/lib/python3.8/site-packages/elasticsearch/_sync/client/utils.py:446\u001b[0m, in \u001b[0;36m_rewrite_parameters.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    444\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/cloudcompa2/.venv/lib/python3.8/site-packages/elasticsearch/_sync/client/__init__.py:2798\u001b[0m, in \u001b[0;36mElasticsearch.msearch\u001b[0;34m(self, searches, body, index, allow_no_indices, ccs_minimize_roundtrips, error_trace, expand_wildcards, filter_path, human, ignore_throttled, ignore_unavailable, max_concurrent_searches, max_concurrent_shard_requests, pre_filter_shard_size, pretty, rest_total_hits_as_int, routing, search_type, typed_keys)\u001b[0m\n\u001b[1;32m   2793\u001b[0m __body \u001b[38;5;241m=\u001b[39m searches \u001b[38;5;28;01mif\u001b[39;00m searches \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m body\n\u001b[1;32m   2794\u001b[0m __headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2795\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccept\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2796\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent-type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/x-ndjson\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2797\u001b[0m }\n\u001b[0;32m-> 2798\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m   2799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2800\u001b[0m \u001b[43m    \u001b[49m\u001b[43m__path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmsearch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_parts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__path_parts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/cloudcompa2/.venv/lib/python3.8/site-packages/elasticsearch/_sync/client/_base.py:271\u001b[0m, in \u001b[0;36mBaseClient.perform_request\u001b[0;34m(self, method, path, params, headers, body, endpoint_id, path_parts)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mperform_request\u001b[39m(\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    257\u001b[0m     method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m     path_parts: Optional[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    265\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ApiResponse[Any]:\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_otel\u001b[38;5;241m.\u001b[39mspan(\n\u001b[1;32m    267\u001b[0m         method,\n\u001b[1;32m    268\u001b[0m         endpoint_id\u001b[38;5;241m=\u001b[39mendpoint_id,\n\u001b[1;32m    269\u001b[0m         path_parts\u001b[38;5;241m=\u001b[39mpath_parts \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[1;32m    270\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m otel_span:\n\u001b[0;32m--> 271\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_perform_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m            \u001b[49m\u001b[43motel_span\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43motel_span\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m         otel_span\u001b[38;5;241m.\u001b[39mset_elastic_cloud_metadata(response\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mheaders)\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/PycharmProjects/cloudcompa2/.venv/lib/python3.8/site-packages/elasticsearch/_sync/client/_base.py:352\u001b[0m, in \u001b[0;36mBaseClient._perform_request\u001b[0;34m(self, method, path, params, headers, body, otel_span)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    350\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTP_EXCEPTIONS\u001b[38;5;241m.\u001b[39mget(meta\u001b[38;5;241m.\u001b[39mstatus, ApiError)(\n\u001b[1;32m    353\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage, meta\u001b[38;5;241m=\u001b[39mmeta, body\u001b[38;5;241m=\u001b[39mresp_body\n\u001b[1;32m    354\u001b[0m     )\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# 'X-Elastic-Product: Elasticsearch' should be on every 2XX response.\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verified_elasticsearch:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;66;03m# If the header is set we mark the server as verified.\u001b[39;00m\n",
      "\u001b[0;31mApiError\u001b[0m: ApiError(502, '<html>\\r\\n<head><title>502 Bad Gateway</title></head>\\r\\n<body>\\r\\n<center><h1>502 Bad Gateway</h1></center>\\r\\n<hr><center>nginx</center>\\r\\n</body>\\r\\n</html>\\r\\n')"
     ]
    }
   ],
   "source": [
    "#try msearch\n",
    "size = 3000 #number of crashes to get\n",
    "radius = 10 #radius in kms\n",
    "#get list of station ID from stations\n",
    "station_ids = stations_copy['Station ID'].tolist()\n",
    "tempresp = tempMSearch(size, station_ids, \"crashes\", radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "#loop through all stations\n",
    "size = 3000 #number of crashes to get\n",
    "radius = 10 #radius in kms\n",
    "crashes = {}\n",
    "empty_stations = []\n",
    "error_stations = []\n",
    "\n",
    "# Define a function to process each station\n",
    "def process_station(station):\n",
    "    station_id = station['Station ID']\n",
    "    crashes[station_id] = crashes.get(station_id, [])\n",
    "    resp = tempFissionCrashesFromStation(station_id, size, radius)\n",
    "    if isinstance(resp, str):\n",
    "        try:\n",
    "            resp = json.loads(resp)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error decoding JSON for station {station_id}\")\n",
    "            return\n",
    "    if resp is None or \"Data\" not in resp or \"Token\" not in resp or \"Status\" not in resp:\n",
    "        error_stations.append(station_id)\n",
    "        return\n",
    "    elif resp[\"Data\"] == [] or resp[\"Token\"] == \"END\" or resp[\"Status\"] != 200:\n",
    "        empty_stations.append(station_id)\n",
    "        return\n",
    "    else:\n",
    "        crashes[station_id].extend(resp[\"Data\"])\n",
    "        while resp and \"Token\" in resp and resp[\"Token\"] != \"END\":\n",
    "            resp = tempFissionStream(token=resp['Token'])\n",
    "            if isinstance(resp, str):\n",
    "                try:\n",
    "                    resp = json.loads(resp)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Error decoding JSON for station {station_id}\")\n",
    "                    return\n",
    "            if resp and resp[\"Data\"]:\n",
    "                crashes[station_id].extend(resp[\"Data\"])\n",
    "\n",
    "# Create a ThreadPoolExecutor with a maximum of 5 threads\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    # Submit each station to the executor\n",
    "    futures = [executor.submit(process_station, station) for index, station in stations_copy.iterrows()]\n",
    "\n",
    "    # Wait for all futures to complete\n",
    "    concurrent.futures.wait(futures)\n",
    "\n",
    "# Print the results\n",
    "print(crashes)\n",
    "print(empty_stations)\n",
    "print(error_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for index, station in stations_copy.iterrows():\n",
    "    station_id = station['Station ID']\n",
    "    print(station_id)\n",
    "    # station_id = \"91306\"\n",
    "    crashes[station_id] = crashes.get(station_id, [])\n",
    "    # params = f\"crashes/{station_id}/{size}/{radius}\"\n",
    "    # resp = get_full_data(params)\n",
    "\n",
    "    resp = tempFissionCrashesFromStation(station_id, size, radius)\n",
    "    if isinstance(resp, str):\n",
    "        try:\n",
    "            resp = json.loads(resp)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error decoding JSON for station {station_id}\")\n",
    "            continue\n",
    "    if resp is None or \"Data\" not in resp or \"Token\" not in resp or \"Status\" not in resp:\n",
    "        error_stations.append(station_id)\n",
    "        continue\n",
    "    elif resp[\"Data\"] == [] or resp[\"Token\"] == \"END\" or resp[\"Status\"] != 200:\n",
    "        empty_stations.append(station_id)\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        crashes[station_id].extend(resp[\"Data\"])\n",
    "        print(\"start\")\n",
    "        while resp and \"Token\" in resp and resp[\"Token\"] != \"END\":\n",
    "            # params = f\"stream/{resp['Token']}\"\n",
    "            # resp = get_full_data(params)\n",
    "            resp = tempFissionStream(token=resp['Token'])\n",
    "            print(resp)\n",
    "            if isinstance(resp, str):\n",
    "                try:\n",
    "                    resp = json.loads(resp)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Error decoding JSON for station {station_id}\")\n",
    "                    continue\n",
    "            if resp and resp[\"Data\"]:\n",
    "                crashes[station_id].extend(resp[\"Data\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_copy = copy.deepcopy(crashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove empty rows from crashes_copy\n",
    "crashes_copy = {k: v for k, v in crashes_copy.items() if v}\n",
    "\n",
    "#convert Dictionary to DataFrame\n",
    "dfs = []\n",
    "for key, value in crashes_copy.items():\n",
    "  # Create a DataFrame from the list of dictionaries\n",
    "    df = pd.json_normalize(value)\n",
    "    #drop location and light_condition colunn\n",
    "    df = df[['_source.crash_date', '_source.severity']]\n",
    "    # Rename the columns\n",
    "    df = df.rename(columns={'_source.crash_date': 'crash_date', '_source.severity': 'severity'})\n",
    "\n",
    "    #format date to DD/MM/YYYY\n",
    "    df['crash_date'] = pd.to_datetime(df['crash_date']).dt.strftime('%d/%m/%Y')\n",
    "    \n",
    "    # Add a new column 'key' with the key value\n",
    "    df['Station ID'] = key\n",
    "    # Add the DataFrame to the list\n",
    "    dfs.append(df)\n",
    "\n",
    "new_df = pd.concat(dfs, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['crash_date'] = pd.to_datetime(new_df['crash_date'], format='%d/%m/%Y')\n",
    "new_df['severity'] = new_df['severity'].astype(int)\n",
    "new_df['Station ID'] = new_df['Station ID'].astype(str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_per_day = new_df.groupby('crash_date').size()\n",
    "crashes_per_day = crashes_per_day.reset_index(name='count')\n",
    "#add Station ID column\n",
    "crashes_per_day['Station ID'] = station_id\n",
    "crashes_per_day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find minimum year\n",
    "min_date = new_df['crash_date'].min().year\n",
    "#find maximum year\n",
    "max_date = new_df['crash_date'].max().year\n",
    "print(min_date)\n",
    "print(max_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting weather data from API\n",
    "weather_data = []\n",
    "for station in new_df['Station ID'].unique():\n",
    "    params = f\"weather/{station}/{min_date}/{max_date}\"\n",
    "    resp = get_full_data(params)\n",
    "    if resp is None or \"Data\" not in resp or \"Status\" not in resp:\n",
    "        error_stations.append(station)\n",
    "        continue\n",
    "    elif resp[\"Data\"] == [] or resp[\"Status\"] != 200:\n",
    "        empty_stations.append(station)\n",
    "        continue\n",
    "    else:\n",
    "        weather_data.extend(resp[\"Data\"])\n",
    "\n",
    "weather_data_copy = copy.deepcopy(weather_data)\n",
    "\n",
    "weather_data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weather_data_copy = copy.deepcopy(weather_data)\n",
    "#convert Dictionary to DataFrame\n",
    "weather_dfs = pd.DataFrame(weather_data_copy)\n",
    "\n",
    "weather_dfs['Station ID'] = station_id\n",
    "#drop created_at, source, Station Name columns\n",
    "weather_dfs = weather_dfs.drop(['created_at', 'source', 'Station Name', 'Pan-Rain'], axis=1)\n",
    "#add dtypes for columns\n",
    "weather_dfs['Date'] = pd.to_datetime(weather_dfs['Date'], format='%d/%m/%Y')\n",
    "weather_dfs['Evapo-Rain'] = weather_dfs['Evapo-Rain'].astype(float)\n",
    "weather_dfs['Rain'] = weather_dfs['Rain'].astype(float)\n",
    "weather_dfs['Max Temp'] = weather_dfs['Max Temp'].astype(float)\n",
    "weather_dfs['Min Temp'] = weather_dfs['Min Temp'].astype(float)\n",
    "weather_dfs['Max Humid'] = weather_dfs['Max Humid'].astype(int)\n",
    "weather_dfs['Min Humid'] = weather_dfs['Min Humid'].astype(int)\n",
    "weather_dfs['WindSpeed'] = weather_dfs['WindSpeed'].astype(float)\n",
    "weather_dfs['UV'] = weather_dfs['UV'].astype(float)\n",
    "weather_dfs['Station ID'] = weather_dfs['Station ID'].astype(str)\n",
    "#drop rows with missing/magic values\n",
    "#drop Min Temp, Max Temp\t= -999\n",
    "weather_dfs = weather_dfs[weather_dfs['Min Temp'] != -999.0]\n",
    "weather_dfs = weather_dfs[weather_dfs['Max Temp'] != -999.0]\n",
    "#drop Rain, Evapo-Rain, Max Humid, Min Humid = -1.0\n",
    "weather_dfs = weather_dfs[weather_dfs['Rain'] != -1.0]\n",
    "weather_dfs = weather_dfs[weather_dfs['Evapo-Rain'] != -1.0]\n",
    "weather_dfs = weather_dfs[weather_dfs['Max Humid'] != -1.0]\n",
    "weather_dfs = weather_dfs[weather_dfs['Min Humid'] != -1.0]\n",
    "weather_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = new_df.merge(weather_dfs, left_on=['Station ID', 'crash_date'], right_on=['Station ID', 'Date'], how='inner')\n",
    "\n",
    "#drop Date and crash_date columns\n",
    "merged_df = merged_df.drop(['Date', 'crash_date'], axis=1)\n",
    "\n",
    "#perform label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "merged_df['Station ID'] = label_encoder.fit_transform(merged_df['Station ID'])\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select only the numerical columns\n",
    "numerical_columns = merged_df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Calculate the correlation between 'severity' and the numerical columns\n",
    "correlation = numerical_columns.corr()['severity'].sort_values()\n",
    "\n",
    "# Print the correlation\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform split on the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = merged_df.drop('severity', axis=1)\n",
    "y = merged_df['severity']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#perform linear regression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "#evaluate the model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "#evaluate the model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "#evaluate the model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "#evaluate the model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "#evaluate the model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "#evaluate the model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "#evaluate the model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "model = MLPClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "#evaluate the model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = crashes_per_day.merge(weather_dfs, left_on=['Station ID', 'crash_date'], right_on=['Station ID', 'Date'], how='inner')\n",
    "# merged_df = merged_df.merge(crashes_per_day, on=['Station ID', 'crash_date'], how='inner')\n",
    "#drop Date and crash_date columns\n",
    "merged_df = merged_df.drop(['Date', 'crash_date','Station ID'], axis=1)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the numerical columns\n",
    "numerical_columns = merged_df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Calculate the correlation between 'severity' and the numerical columns\n",
    "correlation = numerical_columns.corr()['count'].sort_values()\n",
    "\n",
    "# Print the correlation\n",
    "print(merged_df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(numerical_columns.corr(), annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Assuming you have two sets of observations, obs1 and obs2\n",
    "obs1 = [1, 2, 3, 4, 5]\n",
    "obs2 = [2, 3, 4, 5, 6]\n",
    "\n",
    "t_stat, p_value = stats.ttest_ind(obs1, obs2)\n",
    "\n",
    "print(\"P-value: \", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
