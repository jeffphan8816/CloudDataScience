{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import ipywidgets as widgets\n",
    "from ipyleaflet import Map, Marker, Heatmap\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "from functools import partial\n",
    "import random\n",
    "import os\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "RUN_FROM = 'uni_wifi' #'bastion'\n",
    "\n",
    "if RUN_FROM == 'bastion' : URL, HEADERS = 'http://fission:31001/', None\n",
    "if RUN_FROM == 'uni_wifi': URL, HEADERS =  'http://172.26.135.52:9090/', {'HOST': 'fission'}\n",
    "\n",
    "WEATHER_NUM_COL = ['UV', 'Min Temp', 'Max Temp', 'WindSpeed', 'Min Humid', 'Max Humid', 'Rain', 'Pan-Rain', 'Evapo-Rain']\n",
    "\n",
    "\n",
    "FISSION_URL = 'http://172.26.135.52:9090/'\n",
    "FISSION_HEADERS = {'HOST': 'fission'}\n",
    "\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_data(params):\n",
    "    # Define search query (optional, can be empty to retrieve all documents)\n",
    "    data = []\n",
    "    max_retries = 3\n",
    "    retry_delay = 5  # seconds\n",
    "    timeout = 60  # seconds\n",
    "\n",
    "    for _ in range(max_retries):\n",
    "        try:\n",
    "            res = requests.get(f\"{FISSION_URL}/{params}\", headers=FISSION_HEADERS, timeout=timeout)\n",
    "            if res.status_code != 200:\n",
    "                print(res.text)\n",
    "                return None\n",
    "            data = json.loads(res.text)\n",
    "            return data\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Connection error: {e}\")\n",
    "            print(\"Retrying in 5 seconds...\")\n",
    "            time.sleep(retry_delay)\n",
    "    \n",
    "    print(\"Max retries exceeded. Unable to retrieve data.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def clean_data(crashes_df):\n",
    "    crashes_df_copy =  crashes_df.copy(deep=True)\n",
    "    # drop unnecessary columns\n",
    "    crashes_df_copy = crashes_df_copy.drop(columns=['_index', '_id', '_score'])\n",
    "    # rename columns _source.light_condition to light_condition, _source.crash_date to crash_date, _source.severity to severity\n",
    "    crashes_df_copy = crashes_df_copy.rename(columns={\"_source.light_condition\": \"light_condition\", \"_source.crash_date\": \"crash_date\", \"_source.severity\": \"severity\",\"_source.location\": \"location\"})\n",
    "    #convert crash_date to datetime DD/MM/YYYY\n",
    "    crashes_df_copy['crash_date'] = pd.to_datetime(crashes_df_copy['crash_date']).dt.strftime('%d/%m/%Y')\n",
    "    #drop rows with missing/magic values (severity == -1)\n",
    "    crashes_df_copy = crashes_df_copy[crashes_df_copy['severity'] != -1]\n",
    "    #convert location array to 2 columns\n",
    "    crashes_df_copy = pd.concat([crashes_df_copy, crashes_df_copy['location'].apply(pd.Series)], axis=1)\n",
    "    # rename columns 0 to Latitude, 1 to Longitude\n",
    "    crashes_df_copy = crashes_df_copy.rename(columns={0: \"longitude\", 1: \"latitude\"})\n",
    "    crashes_df_copy['severity'] = crashes_df_copy['severity'].astype(int)\n",
    "    crashes_df_copy['Station ID'] = crashes_df_copy['Station ID'].astype(str)\n",
    "    crashes_df_copy['severity_normalized'] = scaler.fit_transform(crashes_df_copy[['severity']])\n",
    "    return crashes_df_copy\n",
    "\n",
    "\n",
    "def plot_trend(crashes_df_copy2):\n",
    "    # Convert the crash_date to datetime format\n",
    "    crashes_df_copy2['crash_date'] = pd.to_datetime(crashes_df_copy2['crash_date'], format='%d/%m/%Y')\n",
    "\n",
    "    # Extract year and month from the crash_date\n",
    "    crashes_df_copy2['year'] = crashes_df_copy2['crash_date'].dt.year\n",
    "    crashes_df_copy2['month'] = crashes_df_copy2['crash_date'].dt.month\n",
    "\n",
    "    # Group by year and month and count the number of crashes\n",
    "    monthly_crashes = crashes_df_copy2.groupby(['month']).size().reset_index(name='crash_count')\n",
    "\n",
    "    # Create a date column for plotting\n",
    "    # monthly_crashes['date'] = pd.to_datetime(monthly_crashes[['month']].assign(day=1))\n",
    "\n",
    "    \n",
    "    # Plot the trend line\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sns.lineplot(data=monthly_crashes, x='month', y='crash_count', marker='o')\n",
    "    plt.title('Monthly Trend of Crash Counts per Month')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Number of Crashes')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def update_plot(plot_output, df):\n",
    "    with plot_output:\n",
    "        #remove all current plot\n",
    "        plt.clf()\n",
    "        # Convert the crash_date to datetime format\n",
    "        df['crash_date'] = pd.to_datetime(df['crash_date'], format='%d/%m/%Y')\n",
    "\n",
    "        # Extract year and month from the crash_date\n",
    "        df['year'] = df['crash_date'].dt.year\n",
    "        df['month'] = df['crash_date'].dt.month\n",
    "\n",
    "        # Group by year and month and count the number of crashes\n",
    "        monthly_crashes = df.groupby(['month']).size().reset_index(name='crash_count')\n",
    "\n",
    "        plot_output.clear_output()  # Clear the previous plot\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.lineplot(data=monthly_crashes, x='month', y='crash_count', marker='o')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Value')\n",
    "        # plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def get_closest_station(lat, lon):\n",
    "    resp = requests.get(URL+f'stations/{lon}/{lat}').json()\n",
    "    if \"Data\" not in resp:\n",
    "        print(\"Server error - unable to make a request for station data\")\n",
    "    elif resp[\"Data\"] == [] or resp[\"Status\"] != 200:\n",
    "        print(\"Error fetching data - no data available\")\n",
    "    else:\n",
    "        return resp['Data']['Station ID'], resp['Data']['location']\n",
    "    return None, None\n",
    "def clean_weather_data(resp):\n",
    "    if resp is None or \"Data\" not in resp or \"Status\" not in resp:\n",
    "        print(\"Server error - unable to make a request for weather data\")\n",
    "    elif resp[\"Data\"] == [] or resp[\"Status\"] != 200:\n",
    "        print(\"Error fetching data - no data available\")\n",
    "    else: \n",
    "        return resp[\"Data\"]\n",
    "    return None\n",
    "        \n",
    "\n",
    "def get_current_weather(lat, lon):\n",
    "    resp  = get_closest_station(lat, lon)    \n",
    "    resp = get_full_data(f'current-weather?id={resp[0]}')\n",
    "    return clean_weather_data(resp)\n",
    "\n",
    "def get_prediction(weather_data):\n",
    "    try:\n",
    "        # Build query\n",
    "        predictors_col = ['UV', 'WindSpeed', 'MaxTemp', 'MinTemp', 'Rain', 'EvapoRain']\n",
    "        predictors = {}\n",
    "        \n",
    "        predictors['UV'] = round(random.uniform(0, 2),1)\n",
    "        predictors['WindSpeed'] = float(int(weather_data['Wind Speed (km/h)']))\n",
    "        predictors['MaxTemp'] = weather_data['Temp'] if float(weather_data['Temp']) > float(weather_data['Apparent Temp']) else weather_data['Apparent Temp']\n",
    "        predictors['MinTemp'] = weather_data['Apparent Temp'] if float(weather_data['Temp']) <= float(weather_data['Apparent Temp']) else weather_data['Temp']\n",
    "        predictors['Rain'] = weather_data['Rain']\n",
    "        predictors['EvapoRain'] = round(random.uniform(1.6, 2.5),1)\n",
    "\n",
    "        query = ''\n",
    "        for field in predictors_col:\n",
    "            query += str(predictors[field]) + ','\n",
    "        res = requests.get(os.path.join(URL, 'models', 'crash_weather_LogisticRegression'), params={'predictors': query[:-1]}, headers=HEADERS)\n",
    "        print(\"Severity Prediction: \" + str(json.loads(res.text)['prediction']))\n",
    "    except:\n",
    "        print('Error Getting Prediction')\n",
    "\n",
    "# Define a function to update the circle location based on the marker location\n",
    "def update_location(change, plot_output):\n",
    "    clear_output()\n",
    "    # Summary of the study\n",
    "    LOCATION = change['new']\n",
    "    RADIUS = slider.value\n",
    "    SIZE = 1000\n",
    "    resp = requests.get(URL+f'stations/{LOCATION[1]}/{LOCATION[0]}').json()\n",
    "    STATION_ID = resp['Data']['Station ID']\n",
    "    STATION_LOCATION = resp['Data']['location']\n",
    "    params = f\"crashes/{STATION_ID}/{SIZE}/{RADIUS}\"\n",
    "    resp = get_full_data(params)\n",
    "    dfs = []\n",
    "    if resp is None or \"Data\" not in resp or \"Token\" not in resp or \"Status\" not in resp:\n",
    "        print(\"Server error - unable to make a request for crash data\")\n",
    "    elif resp[\"Data\"] == [] or resp[\"Token\"] == \"END\" or resp[\"Status\"] != 200:\n",
    "        print(\"Error fetching data - no data available\")\n",
    "    else:\n",
    "        while resp and \"Token\" in resp and resp[\"Token\"] != \"END\":\n",
    "            temp = pd.json_normalize(resp[\"Data\"])\n",
    "            temp['Station ID'] = STATION_ID\n",
    "            dfs.append(temp)\n",
    "            params = f\"stream/{resp['Token']}\"\n",
    "            resp = get_full_data(params)\n",
    "    if dfs == []:\n",
    "        print(\"No data available\")\n",
    "    else:\n",
    "        crashes_df_tmp = pd.concat(dfs, ignore_index=True)\n",
    "        crashes_df2 = clean_data(crashes_df_tmp)\n",
    "        # Create a list of locations with severity\n",
    "        heatmap_data = crashes_df2[['latitude', 'longitude', 'severity_normalized']].values.tolist()\n",
    "        crashes_df = crashes_df2\n",
    "        heatmap.locations = heatmap_data\n",
    "        update_plot(plot_output, crashes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where do you live? How many crashes have happened in your neighborhood?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616467ef77a24e04b09d0e3480fb7344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntSlider(value=20, description='Radius in km', max=50), Map(center=[-41.55381099217959, 147.11â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the map\n",
    "center = (-41.55381099217959, 147.1123612984353)\n",
    "m = Map(center=center, zoom=10)\n",
    "\n",
    "# Create a draggable marker\n",
    "marker = Marker(location=center, draggable=True)\n",
    "m.add_layer(marker)\n",
    "\n",
    "# Create a slider for the radius\n",
    "slider = widgets.IntSlider(description='Radius in km', min=0, max=50, value=20)\n",
    "\n",
    "heatmap = Heatmap(\n",
    "    locations=[],\n",
    "    radius=10,   # Radius of each point of the heatmap\n",
    "    blur=10,     # Amount of blur\n",
    "    max_zoom=1,  # Maximum zoom level\n",
    ")\n",
    "# Add the heatmap layer to the map\n",
    "m.add_layer(heatmap)\n",
    "# Link the marker location change to the update_circle_location function\n",
    "plot_output = widgets.Output()\n",
    "crashes_df = pd.DataFrame()\n",
    "on_marker_moved_with_params = partial(update_location, plot_output=plot_output)\n",
    "\n",
    "marker.observe(on_marker_moved_with_params, names='location')\n",
    "# Display the map and the slider\n",
    "display(widgets.VBox([slider, m, plot_output]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Prediction\n",
    "### If you get in a car crash, how severe will it be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Severity Prediction: 0\n"
     ]
    }
   ],
   "source": [
    "current_weather = get_current_weather(marker.location[0], marker.location[1])\n",
    "prediction = get_prediction(current_weather)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
